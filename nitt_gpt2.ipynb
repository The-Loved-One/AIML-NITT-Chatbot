{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fMSfvXzR4jMvjq0OR2rkbrSRCXaYVMlf",
      "authorship_tag": "ABX9TyMGCXjaIFeOv+penSC231Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/The-Loved-One/AIML-NITT-Chatbot/blob/patch-1/nitt_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "noxBzlojQ-Zc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WgtY7827mN5",
        "outputId": "ad3e9cbb-6488-4843-af92-1c6cd458c9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U PyPDF2\n",
        "!pip install python-docx\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "fFHHaPRn8ASb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to read different file types\n",
        "def read_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "def read_word(file_path):\n",
        "    doc = docx.Document(file_path, encoding='windows-1252')\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\", encoding='windows-1252') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def read_documents_from_directory(directory):\n",
        "    combined_text = \"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            combined_text += read_pdf(file_path)\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            combined_text += read_word(file_path)\n",
        "        elif filename.endswith(\".txt\"):\n",
        "            combined_text += read_txt(file_path)\n",
        "    return combined_text"
      ],
      "metadata": {
        "id": "MVGtkXAK8B2D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_chatbot(directory, model_output_path, train_fraction=0.8):\n",
        "    # Read documents from the directory\n",
        "    combined_text = read_documents_from_directory(directory)\n",
        "    combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()  # Remove excess newline characters\n",
        "\n",
        "    # Split the text into training and validation sets\n",
        "    split_index = int(train_fraction * len(combined_text))\n",
        "    train_text = combined_text[:split_index]\n",
        "    val_text = combined_text[split_index:]\n",
        "\n",
        "    # Save the training and validation data as text files\n",
        "    with open(\"train.txt\", \"w\") as f:\n",
        "        f.write(train_text)\n",
        "    with open(\"val.txt\", \"w\") as f:\n",
        "        f.write(val_text)\n",
        "\n",
        "    # Set up the tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
        "\n",
        "    # Prepare the dataset\n",
        "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n",
        "    val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"val.txt\", block_size=128)\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Set up the training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=model_output_path,\n",
        "        overwrite_output_dir=True,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        num_train_epochs=100,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        logging_dir='./logs',\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(model_output_path)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save_pretrained(model_output_path)"
      ],
      "metadata": {
        "id": "w2iGHBYE8DIe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=100):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Create the attention mask and pad token id\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "9um2zN_W8E8V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    directory = \"/content/nitt-dataset\"  # Replace with the path to your directory containing the files\n",
        "    model_output_path = \"/content/nitt-gpt\"\n",
        "\n",
        "    # Train the chatbot\n",
        "    train_chatbot(directory, model_output_path)\n",
        "\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_output_path)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)\n",
        "\n",
        "    # Test the chatbot\n",
        "    prompt = \"tell me about national institute of technology.\"  # Replace with your desired prompt\n",
        "    response = generate_response(model, tokenizer, prompt)\n",
        "    print(\"Generated response:\", response)"
      ],
      "metadata": {
        "id": "3j5DVUwN8Gax"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "VGGx5PXg8Ogg",
        "outputId": "21d632a1-3d8e-4666-8ea0-f9c8b81e2115"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1200 10:14, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.337900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.009400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response: tell me about national institute of technology.  \n",
            "The National Institute of Technology Tiruchirappalli (NIT Tiruchirappalli or NIT Trichy) is a national resear\n",
            "ch deemed university near the city of Tiruchirappalli in Tamil Nadu, India. It was founded as Regional Eng\n",
            "ineering College Tiruchirappalli in 1964 by the governments of India and Tamil Nadu under the affiliation o\n",
            "f the University of Madras. The college was granted deemed university status in 2003 with the approval of\n",
            " the University Grants Commission (UGC), the All India Council for Technical Education (AICTE), and the \n",
            "Government of India and renamed the National Institute of Technology Tiruchirappalli. The college was granted deemed university status in 2007 with the approval of\n",
            " the University Grants Commission (UGC), the All India Council for Technical Education (AICTE), and the \n",
            "Government of India and renamed the National Institute of Technology Coimbatore. The college was granted deemed university status in 2012 with the approval of\n",
            " the University Grants Commission (UGC), the All India Council for Technical Education (AICTE), and the Government of India and renamed the National Institute of Technology Coimbatore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "DZhMF02W8eH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "acwxcRxg8P06"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=250):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Create the attention mask and pad token id\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "moDzRas58ixE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/nitt-gpt\"\n",
        "# Load the fine-tuned model and tokenizer\n",
        "my_chat_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "my_chat_tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "_2gp_xYg8lIj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"Where is the library? And when does the library open?\"  # Replace with your desired prompt\n",
        "# #prompt = \"What is the most promising future technology?\"\n",
        "# response = generate_response(my_chat_model, my_chat_tokenizer, prompt, max_length=100)  #\n",
        "# print(\"Generated response:\", response)\n",
        "\n",
        "while True:\n",
        "    # Take user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # Check if the user wants to quit\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Tokenize and encode the user's input\n",
        "    # input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate a response\n",
        "    # output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "    response = generate_response(my_chat_model, my_chat_tokenizer, user_input, max_length=150)\n",
        "\n",
        "    # Print the chatbot's response\n",
        "    print(\"Chatbot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocsVXEQA8rYQ",
        "outputId": "baa844f1-d7bb-4ecc-8e23-d695b4760a7f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: national institute of technology\n",
            "Chatbot: national institute of technology Tiruchirappalli. NIT Trichy is recog\n",
            "nized as an Institute of National Importance by the Government of India under the National Institutes of T\n",
            "echnology, Science Education and Research (NITSER) Act, 2007 and is one of the members of the Natio\n",
            "nal Institutes of Technology (NITs) system, a group of premier centrally funded technical institutes (CFTIs\n",
            ") governed by the Council of NITSER. The institute is funded by the Ministry of Education (MoE), Govern\n",
            "ment of India; and focuses exclusively on engineering, management, and research. The institute is funded by the Ministry of Education (MoE), Govern\n",
            "ment of India\n",
            "User: how large is the campus\n",
            "Chatbot: how large is the campus?  \n",
            "The campus is about 2.2 km2 and is one of the largest academic campuses in India. The main \n",
            "entrance is located on the southern end of the campus, facing National Highway 67. There is one other e\n",
            "ntrance, popularly called the Staff Gate. The institute's academic facilities are located in the southern half \n",
            "of the campus; these include the department buildings, laboratories and workshops, lecture halls, comput\n",
            "ation centres and so on. The eastern and western wings of the campus are occupied by the physics and chemistry departments respectively by the\n",
            "beginning of the current academic year. The Orion is the lecture hall complex for the undergraduate s\n",
            "tudents and consists of\n",
            "User: student clubs\n",
            "Chatbot: student clubs and organizations.  \n",
            "Information on starting a new club can be obtained from the student affairs office.  \n",
            "  \n",
            "We have a variety of events and festivals throughout the year promoting cultural exchange and student e\n",
            "ngagement.  \n",
            "You can join a club by attending the club fair held at the beginning of each semester or by contacting the \n",
            "club directly.  \n",
            "Yes we have a variety of events and festivals throughout the year promoting cultural exchange and student e\n",
            "ngagement.  \n",
            "You can join a club by attending the club fair held at the beginning of each semester or by contacting the \n",
            "club directly.  \n",
            "Yes we have a variety of events and festivals throughout the year promoting cultural\n",
            "User: quit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRAQvL8SQxE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}